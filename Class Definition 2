import math
import torch
from torch import nn

# ... Import other required functions like 'stable_digamma' and 'safe_sine'.

class LLM(nn.Module):
    """Language Learning Model with optional digamma integration."""

    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout, use_digamma=False):
        super().__init__()
        
        self.use_digamma = use_digamma
        
        # Define the embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # Add positional encoding
        self.pos_encoding = PositionalEncoding(embed_dim)
        
        # Create the transformer encoder
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(embed_dim, 8, hidden_dim, dropout),
            num_layers
        )
        
        # Final linear projection layer
        self.fc = nn.Linear(embed_dim, vocab_size)
        
        # Attention mask creation
        self.attn_mask = None

    def set_attention_mask(self, seq_length):
        """Create an upper triangular matrix for causality constraint."""
        attn_shape = (seq_length, seq_length)
        distance_matrix = torch.diag(torch.ones(seq_length)).to(self.device)
        lower_triangle = torch.tril(distance_matrix, diagonal=-1)
        ones = torch.ones(*attn_shape).to(self.device)
        paddings = torch.cat((lower_triangle, ones), dim=-1)
        self.attn_mask = ~paddings

    def forward(self, inputs):
        batch_size, seq_length = inputs.size()
        
        # Prepare embeddings
        embeddings = self.embedding(inputs)
        
        # Optionally apply digamma
        if self.use_digamma:
            embeddings = stable_digamma(embeddings)
            
        # Safe sine operation
        embeddings = safe_sine(embeddings)
        
        # Add positional encoding
        embeddings = self.pos_encoding(embeddings)
        
        # Set attention mask
        if self.attn_mask is None or self.attn_mask.size(0) != seq_length:
            self.set_attention_mask(seq_length)
        embeddings *= self.attn_mask.unsqueeze(-1).repeat(1, 1, embeddings.size(-1))
        
        # Pass through Transformer Encoder
        encoded_outputs = self.encoder(embeddings)
        
        # Project to output space and compute logits
        logits = self.fc(encoded_outputs[:, 0])
        
        return logits
